DEEP LEARNING – WORKSHEET 3
Q1 to Q8 are MCQs with only one correct answer. Choose the correct option.

1. Which of the following is true about model capacity (where model capacity means the ability of neural network to approximate complex functions)?
A) As dropout ratio increases, model capacity increases
B) As number of hidden layers increase, model capacity increases
C) As learning rate increases, model capacity increases
D) None of the above
Ans: (B)

2. Batch Normalization is helpful because?
A) It is a very efficient backpropagation technique
B) It returns back the normalized mean and standard deviation of weights
C) It normalizes (changes) all the input before sending it to the next layer
D) None of the above
Ans: (C)

3. What if we use a learning rate that’s too large?
A) Network will not converge B) Network will converge
C) either A or B D) None of the above
Ans: (A)

4. What are the factors to select the depth of neural network?
i) Type of neural network (e.g. MLP, CNN etc.)
ii) Input data
iii) Computation power, i.e. Hardware capabilities and software capabilities
iv) Learning Rate
v) The output function to map
A) 1, 2, 4, 5 B) 2, 3, 4, 5
C) 1, 3, 4, 5 D) All of these
Ans: (D)

5. Suppose you have inputs as x, y, and z with values -2, 5, and -4 respectively. You have a neuron ‘q’ and neuron ‘f’ with functions:
q = x + y
f = q * z
Graphical representation of the functions is as follows:
What is the gradient of F with respect to x, y, and z? (use chain rule of derivatives to find the solution)
A) (3, -4, -4) B) (-3, 4, 4)
C) (-4, -4, 3) D) (4, 4, 3)
Ans: 

6. Which of the following statement is the best description of early stopping?
A) Train the network until a local minimum in the error function is reached
B) Simulate the network on a test dataset after every epoch of training. Stop training when the generalization
error starts to increase
C) Add a momentum term to the weight update in the Generalized Delta Rule, so that training converges more
quickly
D) None of the above
Ans: (B)

7. Which gradient descent technique is more advantageous when the data is too big to handle in RAM simultaneously?
A) Mini Batch Gradient Descent B) Stochastic Gradient Descent
C) Full Batch Gradient Descent D) either A or B
Ans: (B)

8. Consider the scenario. The problem you are trying to solve has a small amount of data. Fortunately, you have a pre-trained neural network that was trained on a similar problem. Which of the following methodologies would you choose to make use of this pre-trained network?
A) Freeze all the layers except the last, re-train the last layer
B) Assess on every layer how the model performs and only select a few of them
C) Fine tune the last couple of layers only
D) Re-train the model for the new dataset
Ans: (A)


Q9 and Q10 are MCQs with one or more correct answers. Choose all the correct options.
9. Which of the following neural network training challenge can be solved using batch normalization?
A) Overfitting B) Training is too slow
C) Restrict activations to become too high or low
D) None of these
Ans: (A), (C) 

10. For a binary classification problem, which of the following activations may be used in output layer?
A) ReLU B) sigmoid
C) softmax D) Leaky ReLU
Ans: (A), (B) 

Q11 to Q15 are subjective answer type question. Answer them briefly.
11. What will happen if we do not use activation function in artificial neural networks?
Ans: The neural network will work as a linear regression model if we do not use an activation function. 

12. How does forward propagation and backpropagation work in deep learning?
Ans: Forward propogation - The neural network traverses from input to output and the network related parameters are calculated and stored.  
Backward propogation - here the gradient of the neural network parameters are calculated. Basically the method traverses the network in reverse order, from the output to the input layer. 

13. Explain briefly the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?
Ans: Stochastic -  means “random” - here in the Gradient Descent it randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.

Batch - in one epoch only one step of gradient descent takes place. - here all the training data is taken in to consideration to take a single step. The avg. of the gradients of all the training examples is taken and its mean gradient is used to update the neural network parameters  

Mini-batch - here, the best of Batch and Stochastic is utilised. A batch of a fixed number of training examples which is less than the actual dataset is considered and called as a mini-batch. and mean gradient is considered of these mini batches 

14. What are the main benefits of Mini-batch Gradient Descent?
Ans: 1. Uses one example at a time, and cannot implement the vectorized implementation on it. This can slow down the computations.
2. frequent updates of the parameters
3. use of vectorised implemenations for faster computations 

15. What is transfer learning?
Ans: Transfer learning in machine learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.