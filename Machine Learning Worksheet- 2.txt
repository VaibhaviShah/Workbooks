Machine Learning Worksheet - 2

 In Q1 to Q5, only one option is correct, Choose the correct option: 

1. In which of the following you can say that the model is overfitting? 
A) High R-squared value for train-set and High R-squared value for test-set. 
B) Low R-squared value for train-set and High R-squared value for test-set. 
C) High R-squared value for train-set and Low R-squared value for test-set. 
D) None of the above 
Ans:( (A) High R-squared value for train-set and High R-squared value for test-set. 

2. Which among the following is a disadvantage of decision trees? 
A) Decision trees are prone to outliers. 
B) Decision trees are highly prone to overfitting. 
C) Decision trees are not easy to interpret 
D) None of the above. 
Ans: (B) Decision trees are highly prone to overfitting. 

3. Which of the following is an ensemble technique? 
A) SVM B) Logistic Regression C) Random Forest D) Decision tree 
Ans: (C) Random Forest

4. Suppose you are building a classification model for detection of a fatal disease where detection of the disease is most important. In this case which of the following metrics you would focus on? 
A) Accuracy B) Sensitivity  C) Precision D) None of the above. 
Ans: (A) Accuracy 

5. The value of AUC (Area under Curve) value for ROC curve of model A is 0.70 and of model B is 0.85. Which of these two models is doing better job in classification? 
A) Model A B) Model B C) both are performing equal D) Data Insufficient 
Ans:  (B) Model B

In Q6 to Q9, more than one options are correct, Choose all the correct options: 
6. Which of the following are the regularization technique in Linear Regression?? 
A) Ridge B) R-squared  C) MSE D) Lasso 
Ans: (A) Ridge, (D) Lasso 

7. Which of the following is not an example of boosting technique? 
A) Adaboost B) Decision Tree C) Random Forest D) Xgboost. 
Ans: (B) Decision Tree, (C) Random Forest

8. Which of the techniques are used for regularization of Decision Trees? 
A) Pruning B) L2 regularization C) Restricting the max depth of the tree D) All of the above 
Ans: (A) Pruning, (C) Restricting the max depth of the tree

9. Which of the following statements is true regarding the Adaboost technique? 
A) We initialize the probabilities of the distribution as 1/n, where n is the number of data-points 
B) A tree in the ensemble focuses more on the data points on which the previous tree was not performing well 
C) It is example of bagging technique 
D) None of the above 
Ans: (A) We initialize the probabilities of the distribution as 1/n, where n is the number of data-points, 
(B) A tree in the ensemble focuses more on the data points on which the previous tree was not performing well 


Q10 to Q15 are subjective answer type questions, Answer them briefly. 
10. Explain how does the adjusted R-squared penalize the presence of unnecessary predictors in the model? 
Ans: The adjusted R-squared willl penalize  for adding independent variables (K in the equation) that do not fit the model.

11. Differentiate between Ridge and Lasso Regression. 
Ans: Lasso – L1 Regularization – the penalty function is defined by the sum of the absolute values of the coefficients, while 
Ridge – L2 Regularization – the penalty function is defined by sum of the squares of the coefficients.
 
12. What is VIF? What is the suitable value of a VIF for a feature to be included in a regression modelling? Many regression variables.
Ans: VIF is Variance Inflation Factor. It is the estimate of the volume of multicollinearity in a collection of 
Suitable value of VIF is less than 10. 0-5 is ideal. 

13. Why do we need to scale the data before feeding it to the train the model? 
Ans: We scale the data, so that a machine learning algorithm does not mistake greater values as higher importance values and consider smaller values as the lower values, regardless of the unit of the values.

14. What are the different metrics which are used to check the goodness of fit in linear regression? 
Ans: R-squared, F-test, and the Root Mean Square Error (RMSE) are some metrics used to check the goodness of fit in linear regression. 

15. From the following confusion matrix calculate sensitivity, specificity, precision, recall and accuracy. 
Actual/Predicted 	True 	False 
True 	1000 	50 
False 	250 	1200 
Ans:
Sensitivity = TP / (TP+FN) = 1000 / (1000+1200) = 0.455
Specificity = TN / (TN + FP) = 50 / (50+250) = 0.1667
Precision = TP / (TP + FP)  =  1000/(1000+50) = 0.95
Recall = TP / (TP+FN) = 1000/(1000+1200) = 0.455
Accuracy = (1000 + 50)/(1000+ 50 +250 + 1200) = 0.42

